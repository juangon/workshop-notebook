{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalizing models\n",
    "\n",
    "We're going to learn how to train a predictive model in an interactive notebook and then expose it as a REST service.  The techniques we'll use in this notebook are intended to illustrate the power and flexibility afforded by publishing models as microservices and are not ones you'd use in production.  However, you'll finish this notebook well-equipped to use a production model server of your choice (or to start implementing your own)!\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "The service we're going to use is available [here](https://github.com/willb/simple-model-server/).  This service exposes a couple of routes:\n",
    "\n",
    "- `/model`, which accepts a HTTP POST request with a form payload containing two serialized Python callables, `validator`, which validates that an argument supplied is suitable for the given model, and `predictor`, which does the actual prediction work; and\n",
    "- `/predict`, which accepts a HTTP POST request with a form payload containing a serialized Python object called `args` (input data to score) and returns the result of making a prediction\n",
    "\n",
    "Our service is not quite immutable, but it is *write-once*:  we will only allow users to install a single model without restarting the service.  Restarting the service will enable you to install a new model.\n",
    "\n",
    "## A client library\n",
    "\n",
    "In order to make it simpler to deal with sending serialized Python objects to the service, we'll make use of a very simple client library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "import cloudpickle\n",
    "\n",
    "def publish(baseurl, validator, predictor):\n",
    "    \"\"\" publish a model consisting of two callables (validator, \n",
    "        which validates input data, and predictor, which makes a \n",
    "        prediction) to the model service located at baseurl \"\"\"\n",
    "    val = base64.b64encode(cloudpickle.dumps(validator))\n",
    "    pred = base64.b64encode(cloudpickle.dumps(predictor))\n",
    "    payload = {'validator' : val, 'predictor': pred}\n",
    "    url = \"%smodel\" % baseurl\n",
    "    r = requests.post(url, data=payload)\n",
    "    return r.text\n",
    "\n",
    "def predict(baseurl, args):\n",
    "    \"\"\" make a prediction from the model service located at baseurl \"\"\"\n",
    "    payload = {'args': base64.b64encode(cloudpickle.dumps(args))}\n",
    "    url = \"%spredict\" % baseurl\n",
    "    r = requests.post(url, data=payload)\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setting up Spark\n",
    "\n",
    "We'll now set up a Spark context, as we did in the previous notebook, generate some random data, and train a k-means clustering for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, column, rand, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "as_vector = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "randomDF = spark.range(0, 2048).select((rand() * 2 - 1).alias(\"x\"), (rand() * 2 - 1).alias(\"y\")).select(column(\"x\"), column(\"y\"), as_vector(array(column(\"x\"), column(\"y\"))).alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "K = 7\n",
    "SEED = 0xdea110c8\n",
    "\n",
    "kmeans = KMeans().setK(K).setSeed(SEED).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(randomDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing our model\n",
    "\n",
    "The Spark models are convenient but many of their implementations depend on having access to a Spark context.  We could, of course, have our model service depend upon Spark, but there's another option:  we can create our own lightweight implementation of the model itself.  (This is usually pretty straightforward, since most machine learning models are far more complicated to train than they are to predict from.)\n",
    "\n",
    "We'll start by looking at the actual data contained within the model:  the cluster centers.  The Spark k-means implementation exposes these by the `clusterCenters` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.70752534, -0.46349022]),\n",
       " array([ 0.69432508,  0.43201093]),\n",
       " array([ 0.65751856, -0.47415352]),\n",
       " array([-0.67205974,  0.59460778]),\n",
       " array([-0.0472727 , -0.69547542]),\n",
       " array([ 0.08862141,  0.72747683]),\n",
       " array([-0.1152846 ,  0.07499047])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make a prediction for the k-means model, we need only identify which of the cluster centers is closest to a given point.  We'll calculate the Euclidean distance from each point to the center (using the `norm` of the vector differences) and find the smallest distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "def km_predict(vec):\n",
    "    _, idx = min([(norm(vec - center), idx) for idx, center in enumerate(centers)])\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that predicting the closest center to one of the actual centers gives us the expected result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "for center in centers:\n",
    "    print(km_predict(center))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll actually publish the model.  Before running the cell below, replace `YOUR_HOSTNAME_HERE` with the route assigned to the simple model service in your project.  For example, if your hostname was `simple-model-server-myproject.127.0.0.1.nip.io`, you'd change the line to `hostname = \"simple-model-server-myproject.127.0.0.1.nip.io\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hostname = \"YOUR_HOSTNAME_HERE\"\n",
    "service_url = \"http://%s:8080/\" % hostname\n",
    "\n",
    "def km_validate(args):\n",
    "    \"\"\" returns true if argument has the same number of dimensions as our cluster center \"\"\"\n",
    "    return len(args) == len(centers[0])\n",
    "\n",
    "publish(service_url, km_validate, km_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises \n",
    "\n",
    "1.  Design a REST API for serving multiple versions of multiple models.\n",
    "1.  &starf; What else would you need to change about the simple model service we used in this project in order to use it in production?  (Hint:  to start, consider security, resilience, high-availability, and load-balancing.)\n",
    "1.  REST calls are convenient but can involve expensive serialization and communication.  (This service in particular trades generality for communication overhead!)  Consider a few techniques for adapting this service to support scoring data with lower latency or higher throughput.\n",
    "1.  The landscape of open-source model servers is increasingly competitive.  Identify a few that you'd like to try out and think about how you'd adapt these techniques to use with a production server.\n",
    "1.  &starf; Consider how you'd design a model service for extremely low-latency environments like transaction processing (in which the round-trip cost of an RPC call might be totally unacceptable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
